---
tittle: 强化学习笔记
comments: true
---
<!--注意事项
1. 行内公式：开始美元符号后面不要有空格，结束前不要有空格。
2. 行间公式$$前需要空行，后不要空行且$$不单独一行。
-->

# 强化学习笔记

<!--
# 一些bug

## TensorFlow}
1. 如何在TensorFlow2的环境下使用TensorFlow1的代码？

首先使用tf.compat.v1替换掉所有的tf.调用，并且在导入TensorFlow软件包后添加一行tf.compat.v1.disable\_eager\_execution()函数来关闭eager执行模式。
-->

## 强化学习的随机策略}
### 贪婪策略

#### $\epsilon$-greedy策略

#### 高斯策略
#### 玻尔兹曼策略
## 强化学习算法的演进归纳

| Value-based                         | Policy-Based                                        | Actor-Critic                                  |
|:-----------------------------------:|:---------------------------------------------------:|:---------------------------------------------:|
| Q-learning(1992)                    |                                                     |                                               |
|                                     |                                                     | Actor-Critic(AC,2000)                         |
|                                     | REINFORCEMENT(PG, 2011)                             |                                               |
| Deep Q-learning(DQN,2015)           | Trust Region Policy Optimization(TRPO,2015)         |                                               |
| Prioritized Experience Replay(2015) |                                                     |                                               |
| Dueling DQN(2015)                   |                                                     |                                               |
| Double DQN(2016)                    |                                                     | Asynchronous Advantage Actor-Critic(A3C,2016) |
| Retrace(2016)                       |                                                     | DDPG(2016)                                    |
| Noisy DQN(2017)                     | Proximal Policy Optimization(PPO,2017)              |                                               |
|                                     | Distributed Proximal Policy Optimization(TPPO,2017) |                                               |
|                                     |                                                     | TD3(2018)                                     |
|                                     |                                                     | Soft Actor-Critic(SAC,2018)                   |

这是我之前学习强化学习时从一个文献中摘录到自己笔记中的，现在找不到原作者了，在此标注一下。


## 常见的强化学习算法

强化学习算法大致可以分为基于值函数的(value-based)和基于策略的(policy-based)，遵循“策略评估-策略改进”的迭代框架，在每次迭代中，对当前策略$\pi_t$进行评估：使用当前策略在环境中进行采样，获取轨迹数据，对此数据进行计算或更新$Q^{\pi_t}(s,a)$(或其他计算目标，如状态价值函数)，然后定一新的策略为
$$
\pi_{t+1}(s)=argmax_a Q^{\pi_t}(s,a).
$$

### 基于值函数的方法
#### 动态规划方法
动态规划方法中，智能体与环境的动态关系是已知的，即$P(r_{t+1},s_{t+1}|s_t,a_t)$已知，可以通过求解值函数的方式得到最优值函数，进而得到最优策略。因此动态规划也被称为基于模型的方法。而在无模型的方法中，需要通过蒙特卡洛采样才估计值函数。

#### 蒙特卡洛方法
在强化学习的设置中，如果无法直接计算Q，可以通过蒙特卡洛采样得到累计期望奖励的一个近似，即

$$
Q^{\pi}(s_t,a_t)=\frac{1}{m}\sum_{i=1}^{m}R(\tau_i).
$$

其中 $\tau_i$是从$(s_t,a_t)$之后执行策略$\pi$得到的轨迹数据，$R(\tau_i)$ 是这条轨迹上的累计奖励，如果每次迭代都需要完成采样，则计算量非常大，因此开始了改进。

##### 改进1：增量算法引出蒙特卡洛强化学习算法
$$
Q(s_t,a_t) \leftarrow \frac{c(s_t,a_t)Q(s_t,a_t)+R}{c(s_t,a_t)+1}
$$

此处 $c(s_t,a_t)$ 是状态-动作对的更新次数，于是就得到了经典的蒙特卡洛强化学习算法。不妨对上式进行改写，得到其等价的更新形式：
\begin{equation}\label{for:MCupdate}
	Q(s_t,a_t) \leftarrow Q(s_t,a_t)+\alpha(R-Q(s_t,a_t)) 
\end{equation}

这里 $\alpha=\frac{1}{c(s_t,a_t)+1}$。
##### 改进2：变换参数成为时序差分算法
$\alpha$设置为一个超参数，R通过当前奖赏和下个状态动作对的值函数之和进行近似更新，即$R \approx r_t+\gamma Q(s_{t+1},a_{t+1})$，就得到了著名的时序差分算法：

\begin{equation}\label{for:TD1}
	Q(s_t,a_t) \leftarrow Q(s_t,a_t)+\alpha\left(r_t+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)\right).
\end{equation}

这样一来，不需要等到轨迹运行完就可以实时更新参数，但是引出了一个新的问题， $a_{t+1}$ 如何选取。一种选择是采样轨迹中在下一个状态上实际执行的动作，这样的更新也被称为"on-policy"，对应的就是**SARSA算法**，因为其需要记录下一个状态下实际的执行动作，即训练数据通常由五元组 $s_t,a_t,r_t,s_{t+1},a_{t+1}$ 构成，这也是其名字的由来。另一种选择是，使用当前的策略，为下一个状态计算一个最优动作，即$a_{t+1}=\pi(s_{t+1})$，这样的更新被称为"off-policy"，对应的就是**Q-Learning**。

此处的$ \delta_t=r_t+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t) $也被称为TD偏差.
\subsubsection{改进3：离散化表格到大规模状态-动作空间}
大规模状态-动作空间中，容易出现维度灾难问题，\textbf{值函数估计} 是解决维度灾难的主要手段之一，其主要思想是将状态价值函数或动作价值函数进行参数化，将值函数空间转化为参数空间，达到 \textbf{泛化(Generalization)}的目的。

以动作价值函数Q函数为例，将其参数化为 $\hat{Q}(s,a|\boldsymbol{\theta})$ ，可以发现时序差分算法更新公式为下面目标式的一步随机梯度下降

\begin{equation}\label{for:gradientde}
	J=(y_t-Q(s_t,a_t))^2
\end{equation}

其中$y_t=r_t+\gamma Q(s_{t+1},a_{t+1})$。那么可以通过最小化
\begin{equation}\label{for:object}
J(\boldsymbol{\theta})=(y_t-Q(s_t,a_t|\boldsymbol{\theta}))^2
\end{equation}

来得到参数 $\boldsymbol{\theta}$ 的更新公式，即
\begin{equation}\label{for:paraupdate}
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} +\alpha(r_t+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)) \nabla_{\boldsymbol{\theta}}(\hat{Q}(s,a|\boldsymbol{\theta}))
\end{equation}

\subsubsection{改进4：深度神经网络和经验回放机制得到DQN}
当 $\hat{Q}(s,a|\boldsymbol{\theta})$ 是一个深度神经网络模型时，就得到了经典的 \textbf{DQN} 算法公式。DQN算法有许多变种，效果提升较大的有 \textbf{Double DQN} ， \textbf{Prioritised Replay} 和 \textbf{Dueling Network} 。
\subsection{基于值函数逼近的强化学习方法}
前面的动态规划、蒙特卡洛以及时序差分算法均有一个限制，即状态空间和动作空间不宜太大。因此本节提出利用函数逼近的方法表示值函数。

在表格型强化学习中，值函数对应一张表格。在之函数逼近的方法中，值函数对应一个逼近函数$ \hat{V}(s) $。从数学角度来看，函数逼近方法可以分为参数逼近和非参数逼近，因此强化学习值函数估计可以分为参数化逼近和非参数化逼近。
其中参数化逼近又分为线性参数化逼近和非线性参数化逼近。

本节我们主要介绍参数化逼近。所谓参数化逼近，是指值函数可以由一组参数$\bm{\theta}$来表示，我们将逼近的值函数记为$ \hat{V}(s,\bm{\theta}) $
\subsection{基于直接策略搜索的方法}
参数化的策略 $\pi_{\theta}$ ,定义其优化目标为
\begin{equation}\label{for:paraobject}
	J(\theta)=\int_{\tau}p_{\theta}(\tau)R(\tau)d\tau
\end{equation}

其中 $p_{\theta}(\tau)$ 是策略 $\pi_{\theta}$ 产生轨迹 $\tau$ 的概率，即
\begin{equation}\label{for:guiji}
	p_{\theta}(\tau)=p(s_0)\prod_{i=0}^{T-1}p(s_{i+1}|a_i,s_i)\pi_{\theta}(a_i|s_i)
\end{equation}

而 $R_{\tau}$ 则对应这样一条轨迹对应的累积奖励。由
\begin{equation}\label{for:logdaoshu}
\nabla_{\theta}p_{\theta}(\tau)=p_{\theta}(\tau)\nabla_{\theta}log p_{\theta}(\tau)
\end{equation}

可以得到目标对应的参数 $\theta$ 的导数为
\begin{equation}\label{for:paradaoshu}
\nabla_{\theta}J=\int_{\tau} p_{\theta}(\tau)\nabla_{\theta}log p_{\theta}(\tau) R_{\tau}d\tau
\end{equation}

不难看出，上式是一个期望，因此可以通过采样的方式采集轨迹 $\tau$ ，计算
\begin{equation}\label{for:paraobj1}
	\nabla_{\theta}J=\mathbb{E}_{\tau\sim p_{\theta}(\tau)}\nabla_{\theta}log p_{\theta}(\tau) R_{\tau}.
\end{equation}

其中， $\nabla_{\theta}log p_{\theta}(\tau)$ 可以进一步展开为
\begin{equation}\label{for:tidumubiao}
\nabla_{\theta}log p_{\theta}(\tau)=\nabla_{\theta}(log (p({s_0})+\sum_{i=0}^{T-1}(log \pi_{\theta}(a_i|s_i)+log p(s_{i+1}|a_i,s_i)))=\sum_{i=0}^{T-1}\nabla_{\theta}log \pi_{\theta}(a_i|s_i)
\end{equation}

于是就得到了经典的 \textbf{REINFORCEMENT算法} 的梯度更新公式：
\begin{equation}\label{for:REINFORCEMENT}
\nabla_{\theta}J(\theta)=\mathbb{E}_{\tau\sim p_{\theta}(\tau)}\sum_{i=0}^{T-1}\nabla_{\theta}log \pi_{\theta}(a_i|s_i) R_{\tau}.
\end{equation}

\begin{equation}\label{for:obejectupdate}
	\theta_{t+1}=\theta_t+\alpha\nabla_{\theta} J(\theta)
\end{equation}
对于另一种等价表达，
\begin{equation}\label{for:lianxuxingshi}
J(\theta)=\int_S d^{\pi_{\theta}}(s)\int_{A}\pi_{\theta}(a|s)Q^{\pi_{\theta}}(s,a)dsda
\end{equation}
也可以得到类似的梯度\footnote{笪庆,强化学习实战——强化学习在阿里的技术演进和业务创新[M],电子工业出版社,2018.}。
\begin{equation}\label{for:lianxuxingshi1}
\nabla_{\theta}J(\theta)=\mathbb{E}_{s\sim d^{\pi_{\theta}}(s),a\sim\pi_{\theta}}[\nabla_{\theta} log\pi_{\theta}(a,s)Q^{\pi_{\theta}}(s,a)]
\end{equation}

<!--

## 做市商获利}
## 高频交易做市商}
# 金融数学
## Levy
\subsubsection{Levy过程的三种刻画}
\paragraph{L\'{e}vy-Khintchine公式}

对L\'{e}vy过程的参数估计，参考Mai(2012)\textsuperscript{\ref{Mai(2012)}}。如果$ L_t $是一个L\'{e}vy过程，则必然存在一个特征三元组$ (b,\sigma^2,v)$，且$ b\in \mathbb{R} $，$ \sigma^2 \in \mathbb{R}_{+} $ 和L\'{e}vy测度( L\'{e}vy measure ) $ v \in \mathbb{R} $满足
\begin{equation}\label{for:levycharacteristics1}
	\phi_{L_t}(u):=\mathbb{E}\left[ e^{iu L_t} \right] =e^{t\psi(u)}
\end{equation}
其中
\begin{equation}\label{for:levycharacteristics}
	\psi(u)= ibu-\frac{1}{2}\sigma^2u^2 +\int_{\mathbb {R}}^{}\left(e^{iu x}-1-iu x 1_{|x|\leq1}\right) v(dx),
\end{equation}

\paragraph{L\'{e}vy-It$\hat{o}$分解}

\begin{equation}\label{for:Levy-ito}
L_t=\sigma B_t+at+\int_{|x|\geq 1}xN_t(dx)+\int_{|x|<1}x\left[N_t(dx)-tv(dx)\right]
\end{equation}

$ N_t(dx) $是一种Poisson随机测度，且与布朗运动$ B_t $相互独立。$ \int min\{|x^2|,1 \}v(dx)<\infty $。$ L_t $被分解为布朗运动、常数漂移、复合Poisson过程、纯跳鞅。

Markov过程}
转移半群：
无穷小算子：

-->